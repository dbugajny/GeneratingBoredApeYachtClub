{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models import decoder, encoder, vae\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from constants import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "apes_info = pd.read_csv(APES_INFO_FILEPATH)\n",
    "train_ids = apes_info.loc[apes_info[\"dataset\"] == \"train\", \"image\"].to_list()\n",
    "validation_ids = apes_info.loc[apes_info[\"dataset\"] == \"validation\", \"image\"].to_list()\n",
    "test_ids = apes_info.loc[apes_info[\"dataset\"] == \"test\", \"image\"].to_list()\n",
    "images_ids = sorted([item.stem for item in pathlib.Path(DATA_FILEPATH).iterdir() if item.suffix == \".png\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10000 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=DATA_FILEPATH,\n",
    "        batch_size=1,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        shuffle=False,\n",
    "        labels=images_ids,\n",
    "    )\n",
    "    .unbatch()\n",
    "    .map(lambda x, y: (x / 255, y))\n",
    ")\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def select_x(x, _):\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "decoder_model = decoder.build_decoder(LATENT_DIM)\n",
    "encoder_model = encoder.build_encoder(LATENT_DIM)\n",
    "\n",
    "vae_model = vae.VAE(encoder_model, decoder_model, 100, 1)\n",
    "vae_model.load_weights(\"../data/models/vae/\")\n",
    "\n",
    "for i in range(len(vae_model.layers)):\n",
    "    vae_model.layers[i].trainable = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "data": {
      "text/plain": "23"
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apes_info[\"Eyes\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "bck = pd.concat([pd.get_dummies(apes_info[\"Background\"]), apes_info[[\"dataset\"]]], axis=1)\n",
    "bck_train = bck[bck[\"dataset\"] == \"train\"].drop(columns=\"dataset\")\n",
    "bck_val = bck[bck[\"dataset\"] == \"validation\"].drop(columns=\"dataset\")\n",
    "\n",
    "mth = pd.concat([pd.get_dummies(apes_info[\"Mouth\"]), apes_info[[\"dataset\"]]], axis=1)\n",
    "mth_train = mth[mth[\"dataset\"] == \"train\"].drop(columns=\"dataset\")\n",
    "mth_val = mth[mth[\"dataset\"] == \"validation\"].drop(columns=\"dataset\")\n",
    "\n",
    "hat = pd.concat([pd.get_dummies(apes_info[\"Hat\"]), apes_info[[\"dataset\"]]], axis=1)\n",
    "hat_train = hat[hat[\"dataset\"] == \"train\"].drop(columns=\"dataset\")\n",
    "hat_val = hat[hat[\"dataset\"] == \"validation\"].drop(columns=\"dataset\")\n",
    "\n",
    "eyes = pd.concat([pd.get_dummies(apes_info[\"Eyes\"]), apes_info[[\"dataset\"]]], axis=1)\n",
    "eyes_train = eyes[eyes[\"dataset\"] == \"train\"].drop(columns=\"dataset\")\n",
    "eyes_val = eyes[eyes[\"dataset\"] == \"validation\"].drop(columns=\"dataset\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input((256, 256, 3))\n",
    "enc = encoder_model(inp)\n",
    "concat = tf.keras.layers.Concatenate()([enc[0], enc[1]])\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(concat)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "out1 = tf.keras.layers.Dense(8, activation=\"sigmoid\", name=\"bck\")(x)\n",
    "out2 = tf.keras.layers.Dense(33, activation=\"sigmoid\", name=\"mth\")(x)\n",
    "out3 = tf.keras.layers.Dense(37, activation=\"sigmoid\", name=\"hat\")(x)\n",
    "out4 = tf.keras.layers.Dense(23, activation=\"sigmoid\", name=\"eyes\")(x)\n",
    "mod2 = tf.keras.Model(inp, [out1, out2, out3, out4], name=\"abc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "mod2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "          tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "          tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "          tf.keras.losses.BinaryCrossentropy(from_logits=False)],\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "y_ds_train = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.Dataset.from_tensor_slices(bck_train),\n",
    "        tf.data.Dataset.from_tensor_slices(mth_train),\n",
    "        tf.data.Dataset.from_tensor_slices(hat_train),\n",
    "        tf.data.Dataset.from_tensor_slices(eyes_train),\n",
    "    )\n",
    ")\n",
    "x_ds_train = dataset.filter(lambda _, y: tf.math.reduce_any(y == train_ids)).map(select_x).batch(1)\n",
    "ds_train = tf.data.Dataset.zip((x_ds_train.unbatch(), y_ds_train)).batch(32)\n",
    "\n",
    "\n",
    "y_ds_val = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.Dataset.from_tensor_slices(bck_val),\n",
    "        tf.data.Dataset.from_tensor_slices(mth_val),\n",
    "        tf.data.Dataset.from_tensor_slices(hat_val),\n",
    "        tf.data.Dataset.from_tensor_slices(eyes_val),\n",
    "    )\n",
    ")\n",
    "x_ds_val = dataset.filter(lambda _, y: tf.math.reduce_any(y == validation_ids)).map(select_x).batch(1)\n",
    "ds_val = tf.data.Dataset.zip((x_ds_val.unbatch(), y_ds_val)).batch(32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    219/Unknown - 29s 132ms/step - loss: 0.5521 - bck_loss: 0.1384 - mth_loss: 0.1193 - hat_loss: 0.1186 - eyes_loss: 0.1759 - bck_accuracy: 0.8204 - mth_accuracy: 0.2220 - hat_accuracy: 0.2234 - eyes_accuracy: 0.1434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 11:53:33.884337: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [10000]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-20 11:53:33.884530: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [10000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 44s 200ms/step - loss: 0.5521 - bck_loss: 0.1384 - mth_loss: 0.1193 - hat_loss: 0.1186 - eyes_loss: 0.1759 - bck_accuracy: 0.8204 - mth_accuracy: 0.2220 - hat_accuracy: 0.2234 - eyes_accuracy: 0.1434 - val_loss: 0.4565 - val_bck_loss: 0.0379 - val_mth_loss: 0.1153 - val_hat_loss: 0.1228 - val_eyes_loss: 0.1805 - val_bck_accuracy: 0.9840 - val_mth_accuracy: 0.2773 - val_hat_accuracy: 0.2153 - val_eyes_accuracy: 0.1733\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 44s 200ms/step - loss: 0.4092 - bck_loss: 0.0225 - mth_loss: 0.1091 - hat_loss: 0.1110 - eyes_loss: 0.1665 - bck_accuracy: 0.9841 - mth_accuracy: 0.2929 - hat_accuracy: 0.2426 - eyes_accuracy: 0.1709 - val_loss: 0.4013 - val_bck_loss: 0.0120 - val_mth_loss: 0.1071 - val_hat_loss: 0.1150 - val_eyes_loss: 0.1672 - val_bck_accuracy: 1.0000 - val_mth_accuracy: 0.3587 - val_hat_accuracy: 0.2407 - val_eyes_accuracy: 0.2660\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 0.3750 - bck_loss: 0.0191 - mth_loss: 0.1013 - hat_loss: 0.1029 - eyes_loss: 0.1518 - bck_accuracy: 0.9796 - mth_accuracy: 0.3497 - hat_accuracy: 0.2973 - eyes_accuracy: 0.2631 - val_loss: 0.3661 - val_bck_loss: 0.0111 - val_mth_loss: 0.0990 - val_hat_loss: 0.1064 - val_eyes_loss: 0.1496 - val_bck_accuracy: 0.9987 - val_mth_accuracy: 0.4327 - val_hat_accuracy: 0.3027 - val_eyes_accuracy: 0.3600\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 42s 194ms/step - loss: 0.3401 - bck_loss: 0.0099 - mth_loss: 0.0955 - hat_loss: 0.0948 - eyes_loss: 0.1400 - bck_accuracy: 0.9921 - mth_accuracy: 0.3966 - hat_accuracy: 0.3537 - eyes_accuracy: 0.3279 - val_loss: 0.3381 - val_bck_loss: 0.0144 - val_mth_loss: 0.0934 - val_hat_loss: 0.0960 - val_eyes_loss: 0.1343 - val_bck_accuracy: 0.9913 - val_mth_accuracy: 0.4787 - val_hat_accuracy: 0.3733 - val_eyes_accuracy: 0.4080\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 42s 193ms/step - loss: 0.3168 - bck_loss: 0.0089 - mth_loss: 0.0907 - hat_loss: 0.0861 - eyes_loss: 0.1311 - bck_accuracy: 0.9933 - mth_accuracy: 0.4306 - hat_accuracy: 0.4114 - eyes_accuracy: 0.3791 - val_loss: 0.3278 - val_bck_loss: 0.0342 - val_mth_loss: 0.0849 - val_hat_loss: 0.0874 - val_eyes_loss: 0.1212 - val_bck_accuracy: 0.9360 - val_mth_accuracy: 0.5107 - val_hat_accuracy: 0.4593 - val_eyes_accuracy: 0.4640\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 44s 198ms/step - loss: 0.2969 - bck_loss: 0.0072 - mth_loss: 0.0857 - hat_loss: 0.0799 - eyes_loss: 0.1240 - bck_accuracy: 0.9940 - mth_accuracy: 0.4710 - hat_accuracy: 0.4596 - eyes_accuracy: 0.4086 - val_loss: 0.2927 - val_bck_loss: 0.0208 - val_mth_loss: 0.0799 - val_hat_loss: 0.0787 - val_eyes_loss: 0.1133 - val_bck_accuracy: 0.9727 - val_mth_accuracy: 0.5720 - val_hat_accuracy: 0.5380 - val_eyes_accuracy: 0.4980\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 43s 196ms/step - loss: 0.2922 - bck_loss: 0.0134 - mth_loss: 0.0833 - hat_loss: 0.0753 - eyes_loss: 0.1202 - bck_accuracy: 0.9834 - mth_accuracy: 0.4800 - hat_accuracy: 0.4989 - eyes_accuracy: 0.4201 - val_loss: 0.2620 - val_bck_loss: 0.0038 - val_mth_loss: 0.0769 - val_hat_loss: 0.0725 - val_eyes_loss: 0.1088 - val_bck_accuracy: 1.0000 - val_mth_accuracy: 0.5953 - val_hat_accuracy: 0.5893 - val_eyes_accuracy: 0.5293\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 43s 196ms/step - loss: 0.2730 - bck_loss: 0.0062 - mth_loss: 0.0806 - hat_loss: 0.0704 - eyes_loss: 0.1157 - bck_accuracy: 0.9934 - mth_accuracy: 0.4996 - hat_accuracy: 0.5353 - eyes_accuracy: 0.4441 - val_loss: 0.2507 - val_bck_loss: 0.0088 - val_mth_loss: 0.0737 - val_hat_loss: 0.0644 - val_eyes_loss: 0.1038 - val_bck_accuracy: 0.9973 - val_mth_accuracy: 0.6000 - val_hat_accuracy: 0.6860 - val_eyes_accuracy: 0.5213\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 0.2604 - bck_loss: 0.0056 - mth_loss: 0.0776 - hat_loss: 0.0666 - eyes_loss: 0.1106 - bck_accuracy: 0.9954 - mth_accuracy: 0.5269 - hat_accuracy: 0.5763 - eyes_accuracy: 0.4740 - val_loss: 0.2346 - val_bck_loss: 0.0090 - val_mth_loss: 0.0694 - val_hat_loss: 0.0622 - val_eyes_loss: 0.0940 - val_bck_accuracy: 0.9913 - val_mth_accuracy: 0.6393 - val_hat_accuracy: 0.6907 - val_eyes_accuracy: 0.5720\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 44s 201ms/step - loss: 0.2608 - bck_loss: 0.0106 - mth_loss: 0.0769 - hat_loss: 0.0636 - eyes_loss: 0.1097 - bck_accuracy: 0.9874 - mth_accuracy: 0.5266 - hat_accuracy: 0.5990 - eyes_accuracy: 0.4780 - val_loss: 0.2158 - val_bck_loss: 0.0041 - val_mth_loss: 0.0651 - val_hat_loss: 0.0566 - val_eyes_loss: 0.0900 - val_bck_accuracy: 1.0000 - val_mth_accuracy: 0.6680 - val_hat_accuracy: 0.7113 - val_eyes_accuracy: 0.6013\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x28df43eb0>"
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(ds_train, validation_data=ds_val, epochs=10, batch_size=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"abc\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 256),                2201184   ['input_25[0][0]']            \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 512)                  0         ['encoder[12][0]',            \n",
      " e)                                                                  'encoder[12][1]']            \n",
      "                                                                                                  \n",
      " dense_84 (Dense)            (None, 64)                   32832     ['concatenate_14[0][0]']      \n",
      "                                                                                                  \n",
      " dense_85 (Dense)            (None, 64)                   4160      ['dense_84[0][0]']            \n",
      "                                                                                                  \n",
      " dense_86 (Dense)            (None, 64)                   4160      ['dense_85[0][0]']            \n",
      "                                                                                                  \n",
      " dense_87 (Dense)            (None, 8)                    520       ['dense_86[0][0]']            \n",
      "                                                                                                  \n",
      " dense_88 (Dense)            (None, 33)                   2145      ['dense_86[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2245001 (8.56 MB)\n",
      "Trainable params: 43817 (171.16 KB)\n",
      "Non-trainable params: 2201184 (8.40 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod2.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
