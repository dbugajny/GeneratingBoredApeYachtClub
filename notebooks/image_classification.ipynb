{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models import decoder, encoder, vae, classifier\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from constants import *\n",
    "from utils import data_loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "apes_info = pd.read_csv(APES_INFO_FILEPATH)\n",
    "all_images_ids, train_ids, validation_ids, test_ids = data_loading.get_image_ids(apes_info, pathlib.Path(DATA_FILEPATH))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10000 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = data_loading.load_full_dataset(DATA_FILEPATH, IMAGE_SIZE, all_images_ids)\n",
    "\n",
    "y_train = data_loading.get_feature_dataset(apes_info, FEATURE_NAMES, \"train\")\n",
    "y_validation = data_loading.get_feature_dataset(apes_info, FEATURE_NAMES, \"validation\")\n",
    "y_test = data_loading.get_feature_dataset(apes_info, FEATURE_NAMES, \"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "decoder_model = decoder.build_decoder(LATENT_DIM)\n",
    "encoder_model = encoder.build_encoder(LATENT_DIM)\n",
    "\n",
    "vae_model = vae.VAE(encoder_model, decoder_model, RECONSTRUCTION_LOSS_WEIGHT, KL_LOSS_WEIGHT)\n",
    "vae_model.load_weights(MODEL_VAE_FILEPATH)\n",
    "\n",
    "classifier_model = classifier.build_classifier(encoder_model, N_UNIQUE_FEATURES, FEATURE_NAMES)\n",
    "classifier_model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(from_logits=False)] * len(FEATURE_NAMES),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train = data_loading.load_specific_dataset(dataset, train_ids, None)\n",
    "train_dataset = tf.data.Dataset.zip((x_train, y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "x_validation = data_loading.load_specific_dataset(dataset, validation_ids, None)\n",
    "validation_dataset = tf.data.Dataset.zip((x_validation, y_validation)).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_CLASSIFIER_FILEPATH,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(HISTORY_CLASSIFIER_FILEPATH, append=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 22:43:15.778423: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_20' with dtype bool and shape [7000,44]\n",
      "\t [[{{node Placeholder/_20}}]]\n",
      "2023-05-23 22:43:15.778642: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_20' with dtype bool and shape [7000,44]\n",
      "\t [[{{node Placeholder/_20}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - ETA: 0s - loss: 1.7255 - Mouth_loss: 0.1736 - Background_loss: 0.3897 - Hat_loss: 0.1862 - Eyes_loss: 0.2322 - Clothes_loss: 0.1868 - Fur_loss: 0.2496 - Earring_loss: 0.3074 - Mouth_accuracy: 0.1505 - Background_accuracy: 0.2815 - Hat_accuracy: 0.1616 - Eyes_accuracy: 0.1059 - Clothes_accuracy: 0.1262 - Fur_accuracy: 0.1040 - Earring_accuracy: 0.6740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 22:43:43.700652: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_14' with dtype bool and shape [1500,8]\n",
      "\t [[{{node Placeholder/_14}}]]\n",
      "2023-05-23 22:43:43.700830: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [10000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 43s 779ms/step - loss: 1.7255 - Mouth_loss: 0.1736 - Background_loss: 0.3897 - Hat_loss: 0.1862 - Eyes_loss: 0.2322 - Clothes_loss: 0.1868 - Fur_loss: 0.2496 - Earring_loss: 0.3074 - Mouth_accuracy: 0.1505 - Background_accuracy: 0.2815 - Hat_accuracy: 0.1616 - Eyes_accuracy: 0.1059 - Clothes_accuracy: 0.1262 - Fur_accuracy: 0.1040 - Earring_accuracy: 0.6740 - val_loss: 1.3228 - val_Mouth_loss: 0.1385 - val_Background_loss: 0.2335 - val_Hat_loss: 0.1391 - val_Eyes_loss: 0.1973 - val_Clothes_loss: 0.1259 - val_Fur_loss: 0.2201 - val_Earring_loss: 0.2684 - val_Mouth_accuracy: 0.2080 - val_Background_accuracy: 0.8207 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1660 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1407 - val_Earring_accuracy: 0.7147\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 42s 789ms/step - loss: 1.1819 - Mouth_loss: 0.1240 - Background_loss: 0.1742 - Hat_loss: 0.1217 - Eyes_loss: 0.1802 - Clothes_loss: 0.1089 - Fur_loss: 0.2054 - Earring_loss: 0.2675 - Mouth_accuracy: 0.1983 - Background_accuracy: 0.7222 - Hat_accuracy: 0.2159 - Eyes_accuracy: 0.1305 - Clothes_accuracy: 0.1767 - Fur_accuracy: 0.1209 - Earring_accuracy: 0.6991 - val_loss: 1.1417 - val_Mouth_loss: 0.1284 - val_Background_loss: 0.1120 - val_Hat_loss: 0.1290 - val_Eyes_loss: 0.1871 - val_Clothes_loss: 0.1174 - val_Fur_loss: 0.2103 - val_Earring_loss: 0.2575 - val_Mouth_accuracy: 0.2180 - val_Background_accuracy: 0.8967 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1660 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1307 - val_Earring_accuracy: 0.7147\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 47s 868ms/step - loss: 1.1023 - Mouth_loss: 0.1229 - Background_loss: 0.1057 - Hat_loss: 0.1203 - Eyes_loss: 0.1782 - Clothes_loss: 0.1082 - Fur_loss: 0.2032 - Earring_loss: 0.2638 - Mouth_accuracy: 0.2034 - Background_accuracy: 0.8584 - Hat_accuracy: 0.2210 - Eyes_accuracy: 0.1425 - Clothes_accuracy: 0.1749 - Fur_accuracy: 0.1199 - Earring_accuracy: 0.7004 - val_loss: 1.0580 - val_Mouth_loss: 0.1227 - val_Background_loss: 0.0648 - val_Hat_loss: 0.1232 - val_Eyes_loss: 0.1809 - val_Clothes_loss: 0.1113 - val_Fur_loss: 0.2035 - val_Earring_loss: 0.2516 - val_Mouth_accuracy: 0.2187 - val_Background_accuracy: 0.8927 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1660 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1360 - val_Earring_accuracy: 0.7147\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 44s 815ms/step - loss: 1.0551 - Mouth_loss: 0.1222 - Background_loss: 0.0643 - Hat_loss: 0.1196 - Eyes_loss: 0.1765 - Clothes_loss: 0.1073 - Fur_loss: 0.2019 - Earring_loss: 0.2633 - Mouth_accuracy: 0.2094 - Background_accuracy: 0.9249 - Hat_accuracy: 0.2222 - Eyes_accuracy: 0.1455 - Clothes_accuracy: 0.1829 - Fur_accuracy: 0.1196 - Earring_accuracy: 0.7001 - val_loss: 1.0691 - val_Mouth_loss: 0.1262 - val_Background_loss: 0.0502 - val_Hat_loss: 0.1271 - val_Eyes_loss: 0.1846 - val_Clothes_loss: 0.1156 - val_Fur_loss: 0.2073 - val_Earring_loss: 0.2580 - val_Mouth_accuracy: 0.2180 - val_Background_accuracy: 0.9027 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1513 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1473 - val_Earring_accuracy: 0.7147\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 1.0262 - Mouth_loss: 0.1205 - Background_loss: 0.0438 - Hat_loss: 0.1185 - Eyes_loss: 0.1760 - Clothes_loss: 0.1065 - Fur_loss: 0.1993 - Earring_loss: 0.2616 - Mouth_accuracy: 0.2087 - Background_accuracy: 0.9515 - Hat_accuracy: 0.2261 - Eyes_accuracy: 0.1464 - Clothes_accuracy: 0.1864 - Fur_accuracy: 0.1285 - Earring_accuracy: 0.6995 - val_loss: 1.0441 - val_Mouth_loss: 0.1270 - val_Background_loss: 0.0207 - val_Hat_loss: 0.1273 - val_Eyes_loss: 0.1851 - val_Clothes_loss: 0.1164 - val_Fur_loss: 0.2073 - val_Earring_loss: 0.2603 - val_Mouth_accuracy: 0.2187 - val_Background_accuracy: 0.9933 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1620 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1413 - val_Earring_accuracy: 0.7147\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 47s 874ms/step - loss: 1.0149 - Mouth_loss: 0.1203 - Background_loss: 0.0403 - Hat_loss: 0.1178 - Eyes_loss: 0.1748 - Clothes_loss: 0.1060 - Fur_loss: 0.1972 - Earring_loss: 0.2586 - Mouth_accuracy: 0.2164 - Background_accuracy: 0.9524 - Hat_accuracy: 0.2288 - Eyes_accuracy: 0.1522 - Clothes_accuracy: 0.1850 - Fur_accuracy: 0.1394 - Earring_accuracy: 0.6998 - val_loss: 1.0143 - val_Mouth_loss: 0.1237 - val_Background_loss: 0.0152 - val_Hat_loss: 0.1246 - val_Eyes_loss: 0.1812 - val_Clothes_loss: 0.1124 - val_Fur_loss: 0.2013 - val_Earring_loss: 0.2559 - val_Mouth_accuracy: 0.2120 - val_Background_accuracy: 0.9987 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1540 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1620 - val_Earring_accuracy: 0.7147\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 44s 824ms/step - loss: 0.9969 - Mouth_loss: 0.1197 - Background_loss: 0.0295 - Hat_loss: 0.1170 - Eyes_loss: 0.1745 - Clothes_loss: 0.1057 - Fur_loss: 0.1923 - Earring_loss: 0.2583 - Mouth_accuracy: 0.2072 - Background_accuracy: 0.9694 - Hat_accuracy: 0.2289 - Eyes_accuracy: 0.1538 - Clothes_accuracy: 0.1868 - Fur_accuracy: 0.1497 - Earring_accuracy: 0.6998 - val_loss: 1.0113 - val_Mouth_loss: 0.1242 - val_Background_loss: 0.0117 - val_Hat_loss: 0.1253 - val_Eyes_loss: 0.1825 - val_Clothes_loss: 0.1129 - val_Fur_loss: 0.1970 - val_Earring_loss: 0.2578 - val_Mouth_accuracy: 0.2193 - val_Background_accuracy: 0.9980 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1647 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.1887 - val_Earring_accuracy: 0.7147\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 44s 818ms/step - loss: 0.9832 - Mouth_loss: 0.1189 - Background_loss: 0.0247 - Hat_loss: 0.1167 - Eyes_loss: 0.1737 - Clothes_loss: 0.1049 - Fur_loss: 0.1869 - Earring_loss: 0.2573 - Mouth_accuracy: 0.2109 - Background_accuracy: 0.9742 - Hat_accuracy: 0.2305 - Eyes_accuracy: 0.1531 - Clothes_accuracy: 0.1877 - Fur_accuracy: 0.1727 - Earring_accuracy: 0.6997 - val_loss: 0.9901 - val_Mouth_loss: 0.1225 - val_Background_loss: 0.0099 - val_Hat_loss: 0.1241 - val_Eyes_loss: 0.1799 - val_Clothes_loss: 0.1110 - val_Fur_loss: 0.1875 - val_Earring_loss: 0.2552 - val_Mouth_accuracy: 0.2260 - val_Background_accuracy: 1.0000 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1520 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.2080 - val_Earring_accuracy: 0.7147\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 44s 826ms/step - loss: 0.9750 - Mouth_loss: 0.1188 - Background_loss: 0.0231 - Hat_loss: 0.1166 - Eyes_loss: 0.1731 - Clothes_loss: 0.1046 - Fur_loss: 0.1820 - Earring_loss: 0.2568 - Mouth_accuracy: 0.2148 - Background_accuracy: 0.9758 - Hat_accuracy: 0.2304 - Eyes_accuracy: 0.1550 - Clothes_accuracy: 0.1877 - Fur_accuracy: 0.1810 - Earring_accuracy: 0.6986 - val_loss: 0.9865 - val_Mouth_loss: 0.1227 - val_Background_loss: 0.0090 - val_Hat_loss: 0.1240 - val_Eyes_loss: 0.1802 - val_Clothes_loss: 0.1114 - val_Fur_loss: 0.1830 - val_Earring_loss: 0.2563 - val_Mouth_accuracy: 0.2260 - val_Background_accuracy: 0.9993 - val_Hat_accuracy: 0.2147 - val_Eyes_accuracy: 0.1673 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.2267 - val_Earring_accuracy: 0.7147\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 46s 848ms/step - loss: 0.9683 - Mouth_loss: 0.1181 - Background_loss: 0.0247 - Hat_loss: 0.1160 - Eyes_loss: 0.1724 - Clothes_loss: 0.1047 - Fur_loss: 0.1776 - Earring_loss: 0.2549 - Mouth_accuracy: 0.2232 - Background_accuracy: 0.9737 - Hat_accuracy: 0.2306 - Eyes_accuracy: 0.1577 - Clothes_accuracy: 0.1874 - Fur_accuracy: 0.2088 - Earring_accuracy: 0.7004 - val_loss: 0.9691 - val_Mouth_loss: 0.1210 - val_Background_loss: 0.0071 - val_Hat_loss: 0.1221 - val_Eyes_loss: 0.1779 - val_Clothes_loss: 0.1096 - val_Fur_loss: 0.1763 - val_Earring_loss: 0.2552 - val_Mouth_accuracy: 0.2160 - val_Background_accuracy: 0.9993 - val_Hat_accuracy: 0.2153 - val_Eyes_accuracy: 0.1673 - val_Clothes_accuracy: 0.1807 - val_Fur_accuracy: 0.2607 - val_Earring_accuracy: 0.7147\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(\n",
    "    train_dataset.repeat(STEPS_PER_EPOCH * EPOCHS_CLASSIFIER),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=10,\n",
    "    batch_size=None,\n",
    "    validation_data=validation_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
